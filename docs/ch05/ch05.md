# 第5章 神经网络架构搜索

![]()

&emsp;&emsp;本章我们将在 5.1 节介神经网络架构搜索？在5.2节介绍神经网络架构搜索的基本概念。5.3节介绍搜索空间的概念、5.4节介绍搜索策略、5.5介绍几个低成本的模型性能评估方法。这些内容构成了神经网络架构搜索的主要内容。


## 5.1 神经网络架构搜索介绍

&emsp;&emsp; 在前几章的剪枝和量化，主要是针对已有的模型上所发展出来的技术。那么是否，存在某种方法能够直接得到一个小模型，且该小模型兼顾性能好、参数量小和高效得特点？

&emsp;&emsp; 这个技术就叫做“神经网络架构搜索”(Neural Architecture Search, NAS)。

&emsp;&emsp; 我们还是要回到之前的目标，如何构建一个“有效模型”，需要满足“延时低、存储少、消耗少，同时还要保持一个良好精度“的目标。

![model_goal](images/model_goal.png)

&emsp;&emsp; 而有时候，对于手工设计的方式，往往会比较难以全部兼容。如果能够将上述的目标转换为”优化目标“，并且网络结构能够自适应这种目标，就能够尽可能在一个空间内做好上述的目标。

&emsp;&emsp; 所以“神经网络架构搜索”的方法，在一定程度上，扩展了搜索空间，使得整个网络有更大的空间来完成上述的目标。


## 5.2 神经网络架构搜索的基本概念

### 5.2.1 基础网络模型回顾

&emsp;&emsp; 首先，对之前的基础网络模型进行回顾，有：线性变换操作，卷积操作，分组卷积操作等。

![base_model](images/base_model.png)

&emsp;&emsp; 这些基础的网络模型有对应的参数量和计算量。

### 5.2.2 网络模块介绍

&emsp;&emsp; 现代的深度学习模型，除了基础的网络之外，还有相关的模型架构嵌套设计，比如ResNet的残差结构。

![ResNet_block](images/ResNet_block.png)

&emsp;&emsp; 通过这种连接方式，一方面能够减少计算量，降低模型训练的难度；另一方面也是一种新型且有效的模块化设计。

&emsp;&emsp; 除了ResNet之外，还有一类有效的网络模块是：Transformer中的多头注意力架构：

![Multi_head_pic](images/Multi_head_pic.png)

&emsp;&emsp; 该模块有一个比较好的性质是，多个模块能够并行计算，加速了模型的计算速度。


## 5.3 搜索空间的介绍

&emsp;&emsp; 回到一开始的问题，传统的网络模型设计，主要集中在人工进行相关架构的设计，那么是否能够利用计算机学会设计合适的网络？

&emsp;&emsp; 除此之外，回顾一开始的目标，即参数量少，还保持模型的准确性，是否存在这样的模型呢？

![experiment_show](images/experiment_show.png)

&emsp;&emsp; 从上述的实验效果图可以看到，传统人工设计的模型存在参数越来越大，性能越来越好的趋势，但实际上，在这个基础上也可以看到，存在一些方法，能够在降低的模型参数的前提下与模型参数比较大的模型精度接近。

&emsp;&emsp; 所以，在当前的研究下，神经网络架构搜索确实能够一定程度上满足上述的要求。

### 5.3.1 神经网络架构搜索的基本流程

&emsp;&emsp; 其流程可以被看做是传统神经网络优化的扩展形式。首先先规定一个搜索空间$\mathcal{A}$，然后给定一些搜索策略，能够得到对应的神经网络架构模型，通过数据集评估该模型的性能，再将该结果反馈给搜索策略，给出迭代后的新神经网络架构。

![NAS_flow](images/NAS_flow.png)


### 5.3.2 搜索空间

&emsp;&emsp; 对于一个单独的神经网络来说，其搜索空间为每一个神经元可能的所有取值的组合。而对于神经网络架构搜索的搜索空间来说，则是不同基本网络模型的组合。所以从理论上来说，其组合的可能也为无穷多个。

&emsp;&emsp; 为了方便讨论，对于搜索空间，可以划分为：“单元级搜索空间”和“网络级搜索空间”，前者主要针对不同基础结构进行组合，后者主要针对模块/网络进行组合。

1. 单元级搜索空间

&emsp;&emsp; 以CNN为例：

![NAS_CNN_example](images/NAS_CNN_example.png)

&emsp;&emsp; 为了展示其组合可能性的大小，在这里做一个简单的数学题：假设我们有两个候选类型输入A和B(指不同的输入形状)，有$M$个不同的变换操作(比如线性层，卷积层等)以及$N$种组合隐藏层的操作(比如求和或平均)，如果整个网络有$L$层，那么该搜索空间有多大？

![Space_cell_example](images/Space_cell_example.jpg)


&emsp;&emsp; 如上图所示，对于每一层来说，每一个输入都有两种情况，每一个输入都有$M$种基础模型，以及有$N$种合并方式，所以最后的搜索空间为：

$$
\text{Search Space} = (2*2*M*M*N)^{L} = 4^LM^{2L}N^L。
$$

&emsp;&emsp; 假设我们令，$M=5,N=2,L=5$，也就是有5种基础模型，2种组合方式，以及5层网络层，其最后的搜索空间为$3.2\times10^{11}$，即$10^{11}$的数量级的大小。

2. 网络级搜索空间

&emsp;&emsp; 除此之外，类似ResNet等有效的网络组合，也需要对其做搜索。

![Space_network_exmaple](images/Space_network_exmaple.png)

&emsp;&emsp; 如上图所示，可以针对残差结构中的深度进行搜索；可以对图像的解析度进行搜索；哈可以对每一层的输入输出维度进行搜索。

### 5.3.3 搜索空间与硬件设备之间的关系

&emsp;&emsp; 上述主要针对网络与任务进行神经网络架构搜索空间的讨论，其背后的一个基本假设是**计算资源是无穷的**，即快速计算，无限存储。但是在实际应用中，不同的设备会有不同的计算速度和存储速度。

![device_limitation](images/device_limitation.png)

&emsp;&emsp; 如何能够在考虑设备计算速度、存储限制的情况下，针对硬件设备本身进行网络架构搜索呢？

![device_limitation2](images/device_limitation2.png)

&emsp;&emsp; 为此，就需要针对硬件设备的限制对搜索空间做限制。
![Space_limiation](images/Space_limiation.png)


## 5.4 搜索策略的介绍

&emsp;&emsp; 在规定好神经网络架构的搜索空间以后，为了能够找到符合目标的神经网络架构，就需要不同的搜索策略。在本小节，主要介绍5种搜索策略，分别是：网格搜索(Grid search)、随机搜索(Random search)、强化学习(Reinforcement learning)、梯度下降(Gradient descent)以及进化算法(Evolutionary search)。

1. 网格搜索

&emsp;&emsp; 该方法顾名思义，将不同的组合都列出来能够组成一张“网格”，通过在网格上进行搜索，以找到最好的结果。

![Grid_search_example](images/Grid_search_example.png)

&emsp;&emsp; 以上图为例，将图像解析度和网络宽度作为搜索变量，对其进行网格化处理，通过数据验证能够得到对应的准确度(如图像分类)，则在满足时延限制下的结果进行比较，选择最好的神经网络架构。

2. 随机搜索

&emsp;&emsp; 如果说，网格搜索是按照顺序的方式进行搜索，那么随机搜索在网格搜索的基础上打乱搜索顺序，有可能能够更快地找到合适的神经网络架构。

![Random_search_example](images/Random_search_example.png)

3. 强化学习

&emsp;&emsp; 对于网格搜索和随机搜索来说，其计算量仍然是巨大的。如果能够通过某种学习方式学习网络架构的设计，能够在一定程度上减少计算量，所以利用强化学习尝试对其进行求解。

![Reinforcement_learning_example](images/Reinforcement_learning_example.png)


&emsp;&emsp; 如上图所示的一种强化学习方法。左图展示的是，通过一个网络架构生成器(某种RNN变体)，通过概率采样生成一个网络架构，再将该网络架构在具体的数据集上训练，评估得到准确率，将该准确率反馈到该控制器进行调整。而右图则是规定了生成一个网络架构的顺序。

4. 梯度下降

&emsp;&emsp; 如果能够将不同的层的选择，与最后的目标函数关联起来就能够利用梯度下降来得到一个神经网络架构。

![Gradient_descent_example](images/Gradient_descent_example.png)

&emsp;&emsp; 如上图所示。对每一层确定不同网络层的选择，对每一个选择给定一个概率，当网络传播的时候则通过概率采样得到对应的网络层，再在最后的目标函数对概率和网络层参数进行优化，得到该神经网络架构。

5. 进化算法

&emsp;&emsp; 有时候梯度信息是难以设计和得到的，但我们仍然有一个目标函数，所以可以通过进化算法对其进行优化。

![Evolutionary_search_example](images/Evolutionary_search_example.png)

&emsp;&emsp; 如上图所示，我们希望模型能够既兼顾时延，同时也兼顾准确率。首先，在原来的网络架构中采样出子网络，对其进行训练和评估得到时延和准确率的信息，通过进化算法判断是否需要丢弃还是保留。然后对保留的字网络进行，比如变异，交叉等操作，模拟细胞分裂时基因的行为。最终选择最优的“基因”，即子网络作为最优的神经网络架构结果。


## 5.5 模型性能评估

&emsp;&emsp;  在上几节，对于得到的神经网络架构模型的评估，主要还是通过在在数据集上进行评估得到模型性能的表现，但是这种方法成本会比较大，因为每一次都需要重新训练模型。在本小节，还将介绍几种方法，在降低成本的同时还能够对模型进行评估，想过方法有：权重继承(Inherit weight)和超网络(Hypernetwork)。

1. 权重继承

&emsp;&emsp; 顾名思义，当新的神经网络架构得到的时候，其权重能够从上一个架构继承，使其减少训练成本。

![Inherit_weight_example](images/Inherit_weight_example.png)

&emsp;&emsp; 以上图的两个模型Net2Wider和Net2Deeper为例，两个模型主要是对原始网络做宽和深的拓展搜索。对于Net2Wider来说，其对某一层拓宽以后，其权值也进行了复制，但也要保持输入和输出的一致；对于Net2Deeper来说，其对模型拓宽深度以后，拓宽的模型参数可以从之前的网络中直接映射过去。

2. 超网络

&emsp;&emsp; 超网络，这里以某个工作为例，该工作将神经网络架构和参数看做是某一个生成网络的结果，在某一损失函数下进行优化。

![Hypernetwork_example](images/Hypernetwork_example.png)

&emsp;&emsp; 如上图所示。其过程为，在每一个训练阶段，都从搜索空间随机采样一个神经网络架构。利用上述图中的图传播得到每一个节点的嵌入向量，再利用MLP生成网络参数，最后利用损失函数进行参数优化。

&emsp;&emsp; 所以该网络并不需要额外对其得到的神经网络架构结果进行训练，因为其已经生成了对应的模型参数。


<!-- ## 5.6 以硬件为主的神经网络架构搜索

&emsp;&emsp;

## 5.7 神经网络架构搜索的应用介绍 -->


