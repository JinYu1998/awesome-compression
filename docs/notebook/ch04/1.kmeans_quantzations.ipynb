{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans 量化实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类量化是利用K-means方法得到权重的聚类中心和标签。同时根据聚类中心和标签又可以回推到weight，当然过程中有损失。\n",
    "<div align=\"center\"> <img src=\"../../ch04/images/k-means.jpg\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置\n",
    "\n",
    "首先，我们安装必须的环境，数据集和model使用和前几章相同的minist数据集和LeNet网络。（PS：最好先创建一个单独的conda环境）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing torchprofile...\n",
      "Installing fast-pytorch-kmeans...\n",
      "All required packages have been successfully installed!\n"
     ]
    }
   ],
   "source": [
    "print('Installing torchprofile...')\n",
    "# !pip install torchprofile -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "print('Installing fast-pytorch-kmeans...')\n",
    "# ! pip install fast-pytorch-kmeans -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "# ! pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "# ! pip install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "# ! conda install pytorch::pytorch torchvision torchaudio -c pytorch \n",
    "# pytorch的安装最好去官网找适合自己的版本命令安装\n",
    "print('All required packages have been successfully installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载用到的python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/learning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1077cab70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型和数据集\n",
    "\n",
    "数据集和模型的链接如下：\n",
    "\n",
    "- 模型权重：https://github.com/datawhalechina/awesome-compression/blob/main/docs/notebook/ch02/model.pt\n",
    "- 数据集：https://github.com/datawhalechina/awesome-compression/tree/main/docs/notebook/ch02/data/mnist/MNIST/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LeNet网络\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置归一化\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# 获取数据集\n",
    "train_dataset = datasets.MNIST(root='../ch02/data/mnist', train=True, download=True, transform=transform)  \n",
    "test_dataset = datasets.MNIST(root='../ch02/data/mnist', train=False, download=True, transform=transform)  # train=True训练集，=False测试集\n",
    "\n",
    "# 设置DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型的状态字典\n",
    "checkpoint = torch.load('../ch02/model.pt')\n",
    "# 加载状态字典到模型\n",
    "model.load_state_dict(checkpoint)\n",
    "fp32_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练和验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  criterion: nn.Module,\n",
    "  optimizer: Optimizer,\n",
    "  scheduler: LambdaLR,\n",
    "  callbacks = None\n",
    ") -> None:\n",
    "  model.train()\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    # inputs = inputs.to('mps')\n",
    "    # targets = targets.to('mps')\n",
    "\n",
    "    # Reset the gradients (from the last iteration)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward inference\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update optimizer and LR scheduler\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  extra_preprocess = None\n",
    ") -> float:\n",
    "  model.eval()\n",
    "\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    # inputs = inputs.to('mps')\n",
    "    if extra_preprocess is not None:\n",
    "        for preprocess in extra_preprocess:\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "    # targets = targets.to('mps')\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建两个函数：计算 Flops 和 Model Size\n",
    "\n",
    "- 参数量（ params ）：\n",
    "    参数的数量，通常以M为单位。\n",
    "    params = Kh × Kw × Cin × Cout\n",
    "- 模型大小(模型大小)：\n",
    "    在一般的深度学习的框架中（如 PyTorch ），一般是 32 位存储，即一个参数用 32 个 bit 来存储。所以，一个拥有 1M（这里的M是数量单位一百万）参数量的模型所需要的存储空间大小为：1M * 32bit = 32Mb = 4MB。\n",
    "- 计算量( Flops )：\n",
    "\n",
    "    即浮点运算数，用来衡量算法/模型的复杂度。图通常只考虑乘加操作的数量，而且只考虑Conv和FC等参数层计算量，忽略BN和PReLU等。一般情况下，Conv和FC层也会忽略仅纯加操作的计算量，如偏置加和shortcut残差加等。目前技术只有BN和CNN可以不加偏置。\n",
    "    FLOPs = Kh * Kw * Cin * Cout * H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_flops(model, inputs):\n",
    "    num_macs = profile_macs(model, inputs)\n",
    "    return num_macs\n",
    "\n",
    "def get_model_size(model: nn.Module, data_width=32):\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    \"\"\"\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证 FP32 模型的精度以及模型大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 model has accuracy=97.99%\n",
      "fp32 model has size=0.17 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "fp32_model_accuracy = evaluate(fp32_model, test_loader)\n",
    "fp32_model_size = get_model_size(fp32_model)\n",
    "print(f\"fp32 model has accuracy={fp32_model_accuracy:.2f}%\")\n",
    "print(f\"fp32 model has size={fp32_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建 K-means 量化函数\n",
    "\n",
    "$n$比特的k-means量化将把数据划分为$2^n$个聚类，而相同聚类中的数据将共享相同的权重值。\n",
    "\n",
    "k-means量化将创建一个 codebook ，其中包括：\n",
    "\n",
    "- centroids：$2^n$ 个FP32聚类中心。\n",
    "- labels：一个$n$比特的整数张量，与原始的FP32权重张量具有相同的元素数量。每个整数表示它属于哪个聚类。\n",
    "\n",
    "在推理期间，基于 codebook 生成一个 FP32 张量进行推理：\n",
    "> ***quantized_weight* = *codebook.centroids*\\[*codebook.labels*\\].view_as(weight)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_pytorch_kmeans import KMeans\n",
    "from collections import namedtuple\n",
    "import pdb\n",
    "Codebook = namedtuple('Codebook', ['centroids', 'labels'])\n",
    "def k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n",
    "    \"\"\"\n",
    "    quantize tensor using k-means clustering\n",
    "    :param fp32_tensor:\n",
    "    :param bitwidth: [int] quantization bit width, default=4\n",
    "    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
    "    :return:\n",
    "        [Codebook = (centroids, labels)]\n",
    "            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n",
    "            labels: [torch.(cuda.)LongTensor] cluster label tensor\n",
    "    \"\"\"\n",
    "    if codebook is None:\n",
    "        # 首先计算聚类的中心点个数\n",
    "        # get number of clusters based on the quantization precision\n",
    "        n_clusters = 2**bitwidth\n",
    "        # print(n_clusters)\n",
    "        # 用kmeans算法得到聚类的中心\n",
    "        # use k-means to get the quantization centroids\n",
    "        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n",
    "        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n",
    "        centroids = kmeans.centroids.to(torch.float).view(-1)\n",
    "        codebook = Codebook(centroids, labels)\n",
    "    \n",
    "    # decode the codebook into k-means quantized tensor for inference\n",
    "    # 解码codebook，得到k-means量化后的tensor\n",
    "    quantized_tensor = codebook.centroids[codebook.labels]\n",
    "    \n",
    "    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n",
    "    return codebook\n",
    "\n",
    "def plot_matrix(tensor, ax, title, cmap=ListedColormap(['white'])):\n",
    "    ax.imshow(tensor.cpu().numpy(), vmin=-0.5, vmax=0.5, cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    for i in range(0,tensor.shape[0]):\n",
    "        for j in range(0,tensor.shape[1]):\n",
    "            \n",
    "            text = ax.text(j, i, f'{tensor[i, j].item():.2f}',ha=\"center\", va=\"center\", color=\"k\") \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    bitwidth = 2\n",
    "    test_tensor = torch.tensor([\n",
    "        [-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n",
    "        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n",
    "        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741]])\n",
    "    \n",
    "    # fig, axes = plt.subplots(1,2, figsize=(8, 12))\n",
    "    # ax_left, ax_right = axes.ravel()\n",
    "    \n",
    "    # plot_matrix(test_tensor, ax_left, 'original tensor')\n",
    "    \n",
    "    # num_unique_values_before_quantization = test_tensor.unique().numel()\n",
    "\n",
    "    # codebook_test = k_means_quantize(fp32_tensor=test_tensor, bitwidth=bitwidth)\n",
    "    # # pdb.set_trace()\n",
    "    # num_unique_values_after_quantization = test_tensor.unique().numel()\n",
    "    \n",
    "    # print(f'    target bitwidth: {bitwidth} bits')\n",
    "    # print(f'        num unique values before k-means quantization: {num_unique_values_before_quantization}')\n",
    "    # print(f'        num unique values after  k-means quantization: {num_unique_values_after_quantization}')\n",
    "    # assert num_unique_values_after_quantization == min((1 << bitwidth), num_unique_values_before_quantization)\n",
    "    \n",
    "    # plot_matrix(test_tensor, ax_right, f'{bitwidth}-bit k-means quantized tensor', \\\n",
    "    #                 cmap='tab20c')\n",
    "    # fig.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在FP32模型上进行K-means量化\n",
    "\n",
    "在下边代码中构建的类`KMeansQuantizer`中，我们必须记录`centroids`和`labels`，以便在模型权重改变时应用或更新 `codebooks`。\n",
    "    \n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import parameter\n",
    "class KMeansQuantizer:\n",
    "    def __init__(self, model : nn.Module, bitwidth=4):\n",
    "        self.codebook = self.quantize(model, bitwidth)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model, update_centroids):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.codebook:\n",
    "                if update_centroids:\n",
    "                    self.update_codebook(param, codebook=self.codebook[name])\n",
    "                self.codebook[name] = k_means_quantize(\n",
    "                    param, codebook=self.codebook[name])\n",
    "                \n",
    "    def update_codebook(self,fp32_tensor: torch.Tensor, codebook: Codebook):\n",
    "        \"\"\"\n",
    "        update the centroids in the codebook using updated fp32_tensor\n",
    "        :param fp32_tensor: [torch.(cuda.)Tensor]\n",
    "        :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
    "        \"\"\"\n",
    "        n_clusters = codebook.centroids.numel()\n",
    "        fp32_tensor = fp32_tensor.view(-1)\n",
    "        for k in range(n_clusters):\n",
    "            cluster_points = fp32_tensor[codebook.labels == k]\n",
    "            if cluster_points.numel() > 0:\n",
    "                codebook.centroids[k] = cluster_points.mean()\n",
    "                \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def quantize(model: nn.Module, bitwidth=4):\n",
    "        codebook = dict()\n",
    "        if isinstance(bitwidth, dict):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in bitwidth:\n",
    "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.dim() > 1:\n",
    "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n",
    "        return codebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们看一下不同的bitwidth下，模型量化后的精度和大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that the storage for codebooks is ignored when calculating the model size.\n",
      "k-means quantizing model into 8 bits\n",
      "    8-bit k-means quantized model has size=0.04 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8-bit k-means quantized model has accuracy=74.00%\n",
      "k-means quantizing model into 4 bits\n",
      "    4-bit k-means quantized model has size=0.02 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4-bit k-means quantized model has accuracy=74.00%\n",
      "k-means quantizing model into 2 bits\n",
      "    2-bit k-means quantized model has size=0.01 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2-bit k-means quantized model has accuracy=74.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print('Note that the storage for codebooks is ignored when calculating the model size.')\n",
    "quantizers = dict()\n",
    "for bitwidth in [8, 4, 2]:\n",
    "    print(f'k-means quantizing model into {bitwidth} bits')\n",
    "    quantizer = KMeansQuantizer(model, bitwidth)\n",
    "    quantized_model_size = get_model_size(model, bitwidth)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
    "    quantized_model_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n",
    "    quantizers[bitwidth] = quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练 kmeans 量化模型\n",
    "可以看到上一步中量化后的模型精度大幅下降，因此我们必须执行量化感知训练来恢复精度。\n",
    "\n",
    "centroids的梯度更新公式如下：\n",
    "\n",
    "> $\\frac{\\partial \\mathcal{L} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\frac{\\partial W_{j} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\mathbf{1}(I_{j}=k)$\n",
    "\n",
    " $\\mathcal{L}$ 是损失, $C_k$ 是第 *k* 个 centroids , $I_{j}$ 是权重 $W_{j}$ 的 label 。$\\mathbf{1}()$ 是找对应 label 的函数, 即, $I_{j}==k$.\n",
    "\n",
    "我们用如下公式更新centroids：\n",
    "\n",
    "> $C_k = \\frac{\\sum_{j}W_{j}\\mathbf{1}(I_{j}=k)}{\\sum_{j}\\mathbf{1}(I_{j}=k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means quantizing model into 8 bits\n",
      "    8-bit k-means quantized model has size=0.04 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8-bit k-means quantized model has accuracy=74.00% before quantization-aware training \n",
      "        Quantization-aware training due to accuracy drop=23.99% is larger than threshold=0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KMeansQuantizer.update_codebook() got multiple values for argument 'codebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m epoch \u001b[38;5;241m=\u001b[39m num_finetune_epochs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m accuracy_drop \u001b[38;5;241m>\u001b[39m accuracy_drop_threshold \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_centroids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     model_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     26\u001b[0m     is_best \u001b[38;5;241m=\u001b[39m model_accuracy \u001b[38;5;241m>\u001b[39m best_accuracy\n",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, callbacks)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m epoch \u001b[38;5;241m=\u001b[39m num_finetune_epochs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m accuracy_drop \u001b[38;5;241m>\u001b[39m accuracy_drop_threshold \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     train(model, train_loader, criterion, optimizer, scheduler,\n\u001b[0;32m---> 24\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_centroids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m])\n\u001b[1;32m     25\u001b[0m     model_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     26\u001b[0m     is_best \u001b[38;5;241m=\u001b[39m model_accuracy \u001b[38;5;241m>\u001b[39m best_accuracy\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/learning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36mKMeansQuantizer.apply\u001b[0;34m(self, model, update_centroids)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_centroids:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_codebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook[name] \u001b[38;5;241m=\u001b[39m k_means_quantize(\n\u001b[1;32m     13\u001b[0m         param, codebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook[name])\n",
      "\u001b[0;31mTypeError\u001b[0m: KMeansQuantizer.update_codebook() got multiple values for argument 'codebook'"
     ]
    }
   ],
   "source": [
    "accuracy_drop_threshold = 0.2\n",
    "quantizers_before_finetune = copy.deepcopy(quantizers)\n",
    "quantizers_after_finetune = quantizers\n",
    "\n",
    "for bitwidth in [8, 4, 2]:\n",
    "    quantizer = quantizers[bitwidth]\n",
    "    print(f'k-means quantizing model into {bitwidth} bits')\n",
    "    quantizer.apply(model, update_centroids=False)\n",
    "    quantized_model_size = get_model_size(model, bitwidth)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
    "    quantized_model_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n",
    "    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n",
    "    if accuracy_drop > accuracy_drop_threshold:\n",
    "        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n",
    "        num_finetune_epochs = 5\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        best_accuracy = 0\n",
    "        epoch = num_finetune_epochs\n",
    "        while accuracy_drop > accuracy_drop_threshold and epoch > 0:\n",
    "            train(model, train_loader, criterion, optimizer, scheduler,\n",
    "                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n",
    "            model_accuracy = evaluate(model, test_loader)\n",
    "            is_best = model_accuracy > best_accuracy\n",
    "            best_accuracy = max(model_accuracy, best_accuracy)\n",
    "            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n",
    "            accuracy_drop = fp32_model_accuracy - best_accuracy\n",
    "            epoch -= 1\n",
    "    else:\n",
    "        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
