# 第6章 知识蒸馏

![](images/111.png)

## 3.1 知识蒸馏介绍

&emsp;&emsp;通常情况下，对于同一个机器学习任务，复杂的网络往往具有更多的参数和更大的计算量，但通常表现更优。相反，简单的网络往往参数较少，计算量较小，但其性能可能相对较差一些。知识蒸馏（Knowledge Distillation, KD）是一种将知识从大型(教师)模型迁移到小型(学生)模型的模型压缩技术。它通过学习教师模型的输出(Logits)分布或特征图(Feature Map)来帮助学生模型学习更好，从而提高学生模型的精度和效率。

![](images/kd-1.png)
&emsp;&emsp; 知识蒸馏的基本架构如上图所示,其中教师模型(Teacher Model)是已经训练好的模型,而学生模型(Student Model)则是需要被训练的模型.假设现在的任务是一个图像分类任务,在训练阶段,对于一张训练图片 $\mathbf x$ 及其对应的真实标签 $y$(也称为硬标签Hard Label)而言. 分别将图片 $\mathbf{x}$ 喂入教师和学习模型,得到两个尺寸相同的输出Logits：$\mathbf{u} = [u_1, u_2, ..., u_K]\in \mathbb{R}^{1\times K}$ 和 $\mathbf{v} = [v_1, v_2, ..., v_K] \in \mathbb{R}^{1\times K}$，其中$K$表示分类任务的数据集对应的类别总数。随后，两个Logits经过Softmax函数处理后，分别得到两个模型关于输入 $\mathbf{x}$ 的最终预测 $\mathbf{p} = [p_1, p_2, ..., p_K]$ 和 $\mathbf{q} = [q_1, q_2, ..., q_K]$。由于教师模型的输出通常较为自信，这将会导致未经处理的 $\mathbf{p}$ 倾向于接近一个 one-hot 向量($[0, 0, 1, 0, ..., 0, 0]$).为了软化教师模型的预测概率,通常的做法是在 softmax 函数中引入温度 $\tau$来进行软化,对于软化后的Logits进行KL散度计算 Loss,成为 Soft Loss (软损失), 同时在蒸馏的时候, 学生模型也要收到监督新号(Hard Label)的监督, 此时和正常的分类任务训练的损失没有区别, 直接对学生模型的输出 $\mathbf{v}$ 和 硬标签 $y$ 计算交叉熵损失即可.这部分的损失称为 Hard Loss (硬损失).最终在 Soft Loss 和 Hard Loss 的共同作用下, 来进行模型蒸馏.

## 3.2 温度

&emsp;&emsp;温度是用来“软化”Logits的,以为了更好的传递知识.软化后的预测概率通常称为“软标签”(Soft Label).与Hard Label相比,Soft Label包含了更多信息,因为它反映了不同类别间的相对关系(例如,在同时分类驴和马的情况下,即使某张图片实际上是马,soft label不会像hard label那样仅在马的索引位置为1,其他为0,而是驴的部分也会有一定概率).在实践中,我们通常也会软化学生模型的输出。最终，软化后的教师模型和学生模型的软标签分别为 $\mathbf{p}(\tau) = [p_1(\tau), p_2(\tau), ..., p_K(\tau)]$ 和 $\mathbf{q}(\tau) = [q_1(\tau), q_2(\tau), ..., q_K(\tau)]$.

&emsp;&emsp;$p_i(\tau)$ 和 $q_i(\tau)$ 的定义如下 : 
$$
\begin{aligned}
    p_{i}(\tau) = \frac{\exp(u_i / \tau)}{\sum_{i=1}^K \exp(u_i / \tau)}   \\
    q_{i}(\tau) = \frac{\exp(v_i / \tau)}{\sum_{i=1}^K \exp(v_i / \tau)} 
\end{aligned}
$$
&emsp;&emsp;其中，$\tau$ 表示的是在蒸馏过程中使用的温度.当温度$\tau$ 为1的时候,此时 $\mathbf p(\tau)$ ,$\mathbf q(\tau)$ 的结果和一般的Softmax函数结果一致.

> 这边得插入一个例子

![](images/fig.png)
&emsp;&emsp;温度 $\tau$ 的大小会控制预测概率的软化程度。较小的 $\tau$ 会导致较尖锐的软化效果，而较大的 $\tau$ 则会使软化效果较为平滑。如上图所示,其中最左侧Origin表示的是某个分类网络对任意一张图片的输出Logits的分布.其中横坐标表示的是具体的类别号,而纵轴表示的是网络对于具体某个类型的预测值.而最右侧的是对 Logits 进行 Arg max() 后的结果. 

&emsp;&emsp;可以发现, 当温度很大的时候(例如 $\tau = 14$ ),此时软化后的Logits分布几乎接近一致, 而当温度很小的时候(例如 $\tau = 0.5$),此时软化后的 Logits 分布几乎等价于 Arg max() 的结果.也就是说, 当温度很大的时候, 教师网络软化后的 Logits 接近于平均值, 此时学生模型无法从教师模型那里学习到知识, 因为教师模型的 Logits 对于每一个类别的预测概率都是一致的. 而当温度很小的时候, 此时蒸馏就失去了意义, 因为教师模型传递给学生的知识可以看作和 Hard Label等价.所以选择**合适的温度**对于蒸馏而言是至关重要的. 一般在 CIFAR-10/100 数据集上, 采用的温度是 $4$, 而在 ImageNet 数据集上, 一般使用的温度是 $1$.

## 3.3 知识蒸馏的损失函数

&emsp;&emsp;知识蒸馏的损失函数由软损失和硬损失线性结合构成, 具体的定义如下:

&emsp;&emsp;**软损失**
$$
\mathcal{L}_{\operatorname{KL}} = \operatorname {KL}(\mathbf{q}(\tau), \mathbf{p}(\tau)) = \sum_j \ p_j(\tau) \log \frac{p_j(\tau)}{q_j(\tau)}
$$

&emsp;&emsp;**硬损失**
$$
\mathcal L_{CE} = \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) = \sum _ {j} - y_j \log q_j(1)
$$

&emsp;&emsp;其中 $\mathbf{p}$ 和 $\mathbf{q}$ 分别表示教师模型和学生模型的输出Logits, 而 $\tau$ 表示的是蒸馏所使用的温度. 最终的损失函数为 
$$
Loss = \alpha \cdot \mathcal L_{CE} + \beta \cdot \tau ^ 2 \cdot \mathcal L_{KL}\\
	=  \alpha \cdot \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) + \beta \cdot \tau ^ 2 \cdot \operatorname {KL}(\mathbf{q}(\tau), \mathbf{p}(\tau))
$$
&emsp;&emsp; 一般情况下, 要保持 $\alpha + \beta = 1$. 实践中 $\alpha$ 通常取 $0.1$, 而 $\beta$ 通常取 $0.9$. 至于为什么要对软损失部分的 $\operatorname {KL}$ 乘一个 $\tau ^ 2$, 简单的解释是为了保持软损失和硬损失在梯度上的平衡, 而具体的解释请选择性的阅读 [3.4]()

## 3.4 损失函数部分推导(选修)

### 3.4.1 Softmax函数求导
&emsp;&emsp;假设对于一个任意的 Logits 向量 $\mathbf z = [z_1, z_2, ..., z_{K}]\in \mathbb{R}^{1\times K}$, 其中 $K$ 是数据集的类别数.通过带温度的Softmax函数计算后得到向量 $\mathbf s = [s_1(\tau), s_2(\tau), ..., s_K(\tau)] $ ,其中 $s_i(\tau)$ 的定义为 : 
$$
s_i(\tau) = \frac{e^{z_i/ \tau}}{\sum_{j=1}^K e^{z_j / \tau}}
$$

&emsp;&emsp;对于任意 $z_k \in \mathbf z$ ,   $s_i(\tau)$ 对 $z_k$ 的偏导分为两种情况:

&emsp;&emsp; 当 $i = k$ 时, 有
$$
\begin{align*}
  \frac{\partial s_i(\tau)}{\partial z_k}
  &= \frac{\partial}{\partial z_k}\frac{e^{z_k / \tau}}{\sum_{j=1}^K e^{z_j / \tau}} \\ 
  &= \frac{\frac{\partial}{\partial z_k}e^{z_k / \tau}\ \sum_{j=1}^K e^{z_j / \tau} - e^{z_k / \tau}\ \frac{\partial}{\partial z_k}\sum_{j=1}^K e^{z_j / \tau}}{\left( \sum_{j=1}^K e^{z_j / \tau}\right) ^ 2} \\
  &= \frac{\frac{1}{\tau}\ e^{z_k/ \tau}}{\sum_{j=1}^K e^{z_j / \tau}} - \frac{e^{z_k/ \tau }\ \frac{1}{\tau}\ e^{z_k/ \tau }}{\left( \sum_{j=1}^K e^{z_j / \tau}\right) ^ 2}\\ 
  &= \frac{1}{\tau} (s_k(\tau) - s_k(\tau)\ s_k(\tau)) \\
  &= \frac{1}{\tau}\ s_k(\tau)\ (1 - s_k(\tau))
\end{align*}
$$

&emsp;&emsp; 当 $i \neq k$ 时, 有
$$
\begin{align*}
  \frac{\partial s_i(\tau)}{\partial z_k}
  &= \frac{\partial}{\partial z_k}\frac{e^{z_i / \tau}}{\sum_{j=1}^K e^{z_j / \tau}} \\ 
  &= \frac{\frac{\partial}{\partial z_k}e^{z_i / \tau}\ \sum_{j=1}^K e^{z_j / \tau} - e^{z_i / \tau}\ \frac{\partial}{\partial z_k}\sum_{j=1}^K e^{z_j / \tau}}{\left( \sum_{j=1}^K e^{z_j / \tau}\right) ^ 2} \\
  &= 0 - \frac{e^{z_i/ \tau }\frac{1}{\tau}e^{z_k/ \tau }}{\left(\sum_{j=1}^K e^{z_j / \tau}\right) ^ 2}\\ 
  &= -\frac{1}{\tau}\ s_i(\tau)\ s_k(\tau)
\end{align*}
$$

&emsp;&emsp; 因此, 对于 $\varphi (z_i)$ 对 $z_k$ 偏导有
$$
\frac{\partial s_i(\tau)}{\partial z_k} =
\left\{
\begin{matrix}
\frac{1}{\tau}\ s_k(\tau)\ (1 - s_k(\tau))& \text{if } i = k \\
-\frac{1}{\tau}\ s_i(\tau)\ s_k(\tau) & \text{if } i \neq k
\end{matrix}
\right.
$$

### 3.4.2 硬损失CE求导
&emsp;&emsp;对于学生模型的输出logits $\mathbf{v} = [v_1, v_2, ..., v_K] \in \mathbb{R}^{1\times K}$ 中任意一个 $v_k$ , 硬损失 $ \mathcal L_{CE}$ 对 $v_k$ 的梯度为 :
$$
\begin{align*}
\mathcal L_{CE} &= \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) \\ 
&= \sum _ {j=1}^K - y_j \log q_j(\tau = 1) \\
\frac{\partial \mathcal L_{CE}}{\partial v_k} &= \frac{\partial}{\partial v_k}\sum _ {j=1}^K - y_j \log q_j(\tau = 1)\\
&= \frac{\partial}{\partial v_k}\sum _ {j=1, j\neq k}^K - y_{j} \log q_j(\tau = 1) + \frac{\partial}{\partial v_k} - y_k \log q_k(\tau = 1)\\
&= \sum _ {j=1, j\neq k}^K - y_{j} \frac{1}{q_j(\tau=1)}\ \frac{-1}{\tau=1}q_j(\tau=1)q_k(\tau=1) \\ &\ \ \ \ \ \ - y_k \frac{1}{q_k(\tau=1)}\frac{1}{\tau=1}q_k(\tau=1)(1-q_k(\tau=1))\\
&= \frac{1}{1}(1-y_k)q_k(\tau=1) - \frac{1}{1}y_k(1-q_k(\tau=1))\\
&= q_k(\tau=1) - y_k \\
\end{align*}
$$

### 3.4.3 软损失KL求导
&emsp;&emsp;软损失$\mathcal L_{KL}$为教师模型的软标签$\mathbf p(\tau)$ 和学生模型的软标签 $\mathbf q(\tau)$  的KL散度.$\mathcal L_{KL}$ 对于学生模型的输出中任意一个 $v_k$ 的梯度为
$$
\begin{align*}
\mathcal{L}_{\operatorname{KL}} &= \operatorname {KL}(\mathbf{q}(\tau), \mathbf{p}(\tau)) \\
&= \sum_{j=1}^{K} \ p_j(\tau) \log \frac{p_j(\tau)}{q_j(\tau)} \\
\frac{\partial \mathcal{L}_{\operatorname{KL}}}{\partial v_k} &= \frac{\partial}{\partial v_k} \sum_{j=1}^{K} \ p_j(\tau) \log \frac{p_j(\tau)}{q_j(\tau)} \\
&= \frac{\partial}{\partial v_k} \sum_{j=1}^{K} \left( \ p_j(\tau) \log {p_j(\tau)} - p_j(\tau)\log{q_j(\tau)} \right) \\
&= \frac{\partial}{\partial v_k} \left(\sum_{j=1}^{K} - p_j(\tau)\log{q_j(\tau)}\right) \\
&= \frac{\partial}{\partial v_k} \left( \sum_{j=1, j\neq k} ^ K -p_j(\tau) \log q_j(\tau)  -p_k(\tau) \log q_k(\tau) \right) \\
&= \sum_{j=1, j\neq k}^K \left( -p_j(\tau) \frac{\partial}{\partial v_k} \log q_j(\tau)\right) - \frac{\partial}{\partial v_k} p_k(\tau) \log q_k(\tau)\\
&= \sum_{j=1, j\neq k}^K -\frac{p_j(\tau)}{q_j(\tau)}\left[ -\frac{1}{\tau}q_j(\tau)q_k(\tau) \right] - \frac{p_k(\tau)}{q_k(\tau)}\left[ \frac{1}{\tau} q_k(\tau)(1 - q_k(\tau))\right]\\ 
&= \frac{1}{\tau}  \sum_{j=1, j\neq k}^K p_j(\tau) q_k(\tau) - \frac{1}{\tau} p_k(\tau)(1 - q_k(\tau)) \ \ \ \text{where} \sum_{j=1}^K p_j(\tau)=1\\
&= \frac{1}{\tau} (1 - p_k(\tau))q_k(\tau) - \frac{1}{\tau} p_k(\tau)(1 - q_k(\tau)) \\
&= \frac{1}{\tau} \left[ q_k(\tau) - p_k(\tau)q_k(\tau) - p_k(\tau) + p_k(\tau)q_k(\tau)\right] \\
&= \frac{q_k(\tau) - p_k(\tau)}{\tau}
\end{align*}
$$

### 3.4.4 泰勒逼近
&emsp;&emsp;对于 $e^x$ , 当 $x$ 趋于 0 的时候有 $e^x \approx 1 + x + ...$
&emsp;&emsp;最终, 硬损失CE和软损失KL对于$v_k$ 的梯度为:
$$
\left\{
\begin{matrix}
\frac{\partial \mathcal L_{CE}}{\partial v_k} = q_k(\tau=1) - y_k
\\
\frac{\partial \mathcal{L}_{\operatorname{KL}}}{\partial v_k} = \frac{1}{\tau}(q_k(\tau) - p_k(\tau))
\end{matrix}
\right.
$$

&emsp;&emsp;对于 $\frac{\partial \mathcal L_{CE}}{\partial v_k}$ 展开有 :
$$
\begin{align*}
\frac{\partial \mathcal L_{CE}}{\partial v_k} &= q_k(\tau=1)-y_k\\
&= \frac{e^{v_k}}{\sum_{j=1}^K e^{v_j}} - y_k \\
&\approx \frac{1+v_k}{\sum_{j=1}^K 1 + v_j} - y_k  , \text{where} \sum v_j = 0\\
&= \frac{1+v_k}{K} - y_k\\
\end{align*}
$$
​
&emsp;&emsp;对于 $\frac{\partial \mathcal{L}_{\operatorname{KL}}}{\partial v_k} $ 展开有 :
$$
\begin{align*}
\frac{\partial \mathcal{L}_{\operatorname{KL}}}{\partial v_k} &= \frac{1}{\tau}(q_k(\tau) - p_k(\tau))\\
&= \frac{1}{\tau} (\frac{e^{v_k/\tau}}{\sum_{j=1}^K e^{v_j / \tau}} - \frac{e^{u_k/ \tau}}{\sum_{j=1}^K e^{u_k/\tau}}) \\
&\approx \frac{1}{\tau}(\frac{1 + v_k/\tau}{\sum_{j=1}^K (1 + v_j/\tau)} - \frac{1+u_k/\tau}{\sum_{j=1}^K (1+u_j/ \tau)})\\
&= \frac{1}{\tau}(\frac{v_k/\tau - u_k}{K}) \\
&= \frac{1}{K \ \tau^2} v_k - \frac{u_k}{K\tau}
\end{align*}
$$

&emsp;&emsp;此时可以发现, 硬损失中梯度对于$v_k$ 的部分时软损失的梯度中对于$v_k$部分的 $\tau ^ 2$ 倍, 所以在最终计算损失函数Loss的时候, 需要给 $\mathcal{L}_{\operatorname{KL}}$ 乘上一个 $\tau ^ 2$ 以平衡两个损失之间的梯度.


## 3.5 解耦知识蒸馏

## 3.6 实践

## 3.7 更多KD资料
<!-- - 本文只介绍了基于Logits的知识蒸馏.
- 如果想更加深入的了解KD, 可以参考这篇[综述](https://arxiv.org/abs/2006.05525)
- https://zhuanlan.zhihu.com/p/102038521
- https://zhuanlan.zhihu.com/p/477348100 -->

## 3.8 关于温度的其他讨论
<!-- - [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- Smith, Jane. *Another Reference*. Publisher, Year. -->
