# 第7章 项目实践

<!-- 本章节将通过一个综合类的项目实践融入前面介绍的模型压缩方法帮助学习者更好的理解 -->

&emsp;&emsp;本教程中提到的模型压缩算法，剪枝、量化、蒸馏、神经网络架构搜索，在实际应用中相互垂直，各有自己适用的场景和取舍。然而我们可以将多种模型压缩算法进行结合，以达到更好的效果。

&emsp;&emsp;目前，效果较好，研究较为丰富的模型压缩算法的结合运用主要有：
- 剪枝与量化结合
- 知识蒸馏与量化结合

&emsp;&emsp;另外，最近的一些研究中也提出了一些更加综合的模型压缩算法的结合方法，比如：
- 知识蒸馏与剪枝，量化结合
- 神经网络架构搜索与剪枝，量化结合

&emsp;&emsp;本教程将结合实际中的应用，对于xx任务，通过对xx模型应用不同的压缩方法，并从硬件效率提升，模型能力保留，与算法实现难度的角度进行对比


## 7.1 剪枝与量化结合

&emsp;&emsp;剪枝和量化都可以很好地减少模型权重和激活值中的冗余，从而减少模型的大小和计算量。

&emsp;&emsp;前面的章节提到：

>剪枝（从时机上）可以分为：
> - 训练后剪枝 （静态稀疏性）
> - 训练时剪枝 （动态稀疏性）
> - 训练前剪枝

>而量化主要分为：
> - 训练后量化 PTQ
> - 量化感知训练 QAT

&emsp;&emsp;对于部署和压缩加速已有的模型，常用的解决方案是将训练后剪枝和训练后量化结合。另外，对于大语言模型LLM，在进行迭代剪枝时，也可以使用QLoRA来大大减少微调的内存开销和数据需求。


### 7.1.1 剪枝与PTQ结合

#### 7.1.1.1 初步尝试（简单组合）
&emsp;&emsp;简单直觉上来说，剪枝和量化结合的方法可以有：
&emsp;&emsp;**先剪枝，再量化**：首先对模型进行剪枝，然后对剪枝后的模型进行量化（PTQ）。
&emsp;&emsp;**先量化，再剪枝**：首先对模型进行量化（PTQ），然后对量化后的模型进行剪枝。

```python

```
&emsp;&emsp;然而，对于这样简单的拼凑，会出现一系列问题：
对于**先剪枝，再量化**
- 剪枝之后的模型减少了信息的通道数，导致模型的鲁棒性降低，因而对量化引入的噪声的容忍度降低，导致量化后的模型性能下降。
- 剪通常将权重设置为零，从而产生稀疏性。但是，量化可以使这些零值映射到非零量化值，从而降低稀疏性。尤其是如果量化中进行了微调在剪枝后进行量化感知训练（QAT），在训练过程中（如果没有特殊处理）也会将这些一些零值调整到非零值，从而降低稀疏性。

对于**先量化，再剪枝**
- ...

&emsp;&emsp;因此，我们需要对剪枝和量化的结合进行更深入的研究，以解决这些问题。

#### 7.1.1.2 问题分析与解决

Frederick Tung & Greg Mori（2018）在论文[CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization](https://doi.org/10.1109/cvpr.2018.00821)中提出


Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning


